#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¾®ä¿¡å…¬ä¼—å·æ–‡ç« æŠ“å–å™¨ FastAPI æœåŠ¡
æä¾›ä¸¤ä¸ªAPIæ¥å£ï¼š
1. GET /article/info - è·å–æ–‡ç« ä¿¡æ¯
2. POST /article/save-to-feishu - ä¿å­˜æ–‡ç« åˆ°é£ä¹¦
"""

from fastapi import FastAPI, HTTPException, BackgroundTasks
from pydantic import BaseModel, HttpUrl
from typing import Optional, List, Dict, Any
import uvicorn
from bs4 import BeautifulSoup
import logging

# å¯¼å…¥ç°æœ‰çš„ç±»å’Œé…ç½®
from wechat_article_scraper import WeChatArticleScraper, FeishuBitableClient
import config

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=getattr(logging, config.LOG_LEVEL, logging.INFO),
    format=config.LOG_FORMAT
)
logger = logging.getLogger(__name__)

# åˆ›å»ºFastAPIåº”ç”¨
app = FastAPI(
    title="å¾®ä¿¡å…¬ä¼—å·æ–‡ç« æŠ“å–å™¨API",
    description="æä¾›å¾®ä¿¡å…¬ä¼—å·æ–‡ç« æŠ“å–å’Œé£ä¹¦ä¿å­˜åŠŸèƒ½",
    version="1.0.0"
)

# è¯·æ±‚æ¨¡å‹
class ArticleRequest(BaseModel):
    url: HttpUrl
    include_ocr: Optional[bool] = True
    download_images: Optional[bool] = True

class FeishuSaveRequest(BaseModel):
    url: HttpUrl
    include_ocr: Optional[bool] = True
    download_images: Optional[bool] = True

# å“åº”æ¨¡å‹
class ImageInfo(BaseModel):
    src: str
    alt: str
    title: str

class OCRResult(BaseModel):
    image_url: str
    local_path: str
    ocr_text: str
    alt: str
    title: str

class ArticleInfo(BaseModel):
    url: str
    title: str
    content: str
    account_name: Optional[str] = ""
    publish_date: Optional[str] = ""
    images: List[ImageInfo]
    ocr_results: List[OCRResult]
    image_count: int
    content_length: int

class ApiResponse(BaseModel):
    success: bool
    message: str
    data: Optional[Any] = None

# å…¨å±€æŠ“å–å™¨å®ä¾‹
scraper = WeChatArticleScraper(output_dir=config.OUTPUT_DIR)

@app.get("/")
async def root():
    """æ ¹è·¯å¾„ï¼Œè¿”å›APIä¿¡æ¯"""
    return {
        "message": "å¾®ä¿¡å…¬ä¼—å·æ–‡ç« æŠ“å–å™¨API",
        "version": "1.0.0",
        "endpoints": {
            "/article/info": "GET - è·å–æ–‡ç« ä¿¡æ¯",
            "/article/save-to-feishu": "POST - ä¿å­˜æ–‡ç« åˆ°é£ä¹¦",
            "/docs": "APIæ–‡æ¡£"
        }
    }

@app.get("/article/info", response_model=ApiResponse)
async def get_article_info(url: str, include_ocr: bool = True, download_images: bool = True):
    """
    åŠŸèƒ½1ï¼šè·å–æ–‡ç« ä¿¡æ¯
    
    Args:
        url: å¾®ä¿¡å…¬ä¼—å·æ–‡ç« é“¾æ¥
        include_ocr: æ˜¯å¦è¿›è¡ŒOCRè¯†åˆ«
        download_images: æ˜¯å¦ä¸‹è½½å›¾ç‰‡
    
    Returns:
        åŒ…å«æ–‡ç« æ ‡é¢˜ã€æ­£æ–‡ã€å›¾ç‰‡ç­‰ä¿¡æ¯çš„å“åº”
    """
    try:
        logger.info(f"å¼€å§‹å¤„ç†æ–‡ç« ä¿¡æ¯è¯·æ±‚: {url}")
        
        # è·å–åŸºæœ¬æ–‡ç« å†…å®¹
        article_info = scraper.get_article_content(url)
        if not article_info:
            raise HTTPException(status_code=400, detail="æ— æ³•è·å–æ–‡ç« å†…å®¹ï¼Œè¯·æ£€æŸ¥URLæ˜¯å¦æ­£ç¡®")
        
        # è·å–å…¬ä¼—å·åç§°å’Œå‘å¸ƒæ—¥æœŸ
        try:
            response = scraper.session.get(url, headers=scraper.headers, timeout=30)
            response.raise_for_status()
            soup = BeautifulSoup(response.text, 'html.parser')
            account_name = scraper.extract_account_name(soup)
            publish_date = scraper.extract_publish_date(soup)
        except Exception as e:
            logger.warning(f"è·å–å…¬ä¼—å·ä¿¡æ¯å¤±è´¥: {e}")
            account_name = ""
            publish_date = ""
        
        # å¤„ç†å›¾ç‰‡å’ŒOCRï¼ˆå¦‚æœéœ€è¦ï¼‰
        if download_images and include_ocr:
            for i, img_info in enumerate(article_info['images']):
                try:
                    # ç”Ÿæˆæ–‡ä»¶å
                    file_extension = scraper._get_file_extension(img_info['src'])
                    filename = f"{config.IMAGE_FILENAME_PREFIX}{i+1}{file_extension}"
                    
                    # ä¸‹è½½å›¾ç‰‡
                    image_path = scraper.download_image(img_info['src'], filename)
                    
                    if image_path:
                        # OCRè¯†åˆ«
                        ocr_text = scraper.ocr_image(image_path)
                        
                        # ä¿å­˜OCRç»“æœ
                        article_info['ocr_results'].append({
                            'image_url': img_info['src'],
                            'local_path': image_path,
                            'ocr_text': ocr_text,
                            'alt': img_info['alt'],
                            'title': img_info['title']
                        })
                except Exception as e:
                    logger.error(f"å¤„ç†å›¾ç‰‡ {i+1} å¤±è´¥: {e}")
                    if not config.CONTINUE_ON_ERROR:
                        raise HTTPException(status_code=500, detail=f"å¤„ç†å›¾ç‰‡å¤±è´¥: {e}")
        
        # æ„å»ºå“åº”æ•°æ®
        response_data = ArticleInfo(
            url=article_info['url'],
            title=article_info['title'],
            content=article_info['content'],
            account_name=account_name,
            publish_date=publish_date,
            images=[ImageInfo(**img) for img in article_info['images']],
            ocr_results=[OCRResult(**ocr) for ocr in article_info['ocr_results']],
            image_count=len(article_info['images']),
            content_length=len(article_info['content'])
        )
        
        logger.info(f"æˆåŠŸå¤„ç†æ–‡ç« : {article_info['title']}")
        return ApiResponse(
            success=True,
            message="æ–‡ç« ä¿¡æ¯è·å–æˆåŠŸ",
            data=response_data
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"è·å–æ–‡ç« ä¿¡æ¯å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"æœåŠ¡å™¨å†…éƒ¨é”™è¯¯: {str(e)}")

@app.post("/article/save-to-feishu", response_model=ApiResponse)
async def save_article_to_feishu(request: FeishuSaveRequest, background_tasks: BackgroundTasks):
    """
    åŠŸèƒ½2ï¼šä¿å­˜æ–‡ç« åˆ°é£ä¹¦
    
    Args:
        request: åŒ…å«æ–‡ç« URLå’Œå¤„ç†é€‰é¡¹çš„è¯·æ±‚
        background_tasks: åå°ä»»åŠ¡ï¼ˆç”¨äºå¼‚æ­¥å¤„ç†å›¾ç‰‡ä¸‹è½½å’ŒOCRï¼‰
    
    Returns:
        ä¿å­˜ç»“æœå“åº”
    """
    try:
        logger.info(f"å¼€å§‹å¤„ç†é£ä¹¦ä¿å­˜è¯·æ±‚: {request.url}")
        
        # æ£€æŸ¥é£ä¹¦é…ç½®
        if not config.FEISHU_ENABLED:
            raise HTTPException(status_code=400, detail="é£ä¹¦åŠŸèƒ½æœªå¯ç”¨")
        
        # è·å–æ–‡ç« å†…å®¹
        article_info = scraper.get_article_content(str(request.url))
        if not article_info:
            raise HTTPException(status_code=400, detail="æ— æ³•è·å–æ–‡ç« å†…å®¹ï¼Œè¯·æ£€æŸ¥URLæ˜¯å¦æ­£ç¡®")
        
        # è·å–å…¬ä¼—å·åç§°å’Œå‘å¸ƒæ—¥æœŸ
        try:
            response = scraper.session.get(str(request.url), headers=scraper.headers, timeout=30)
            response.raise_for_status()
            soup = BeautifulSoup(response.text, 'html.parser')
            account_name = scraper.extract_account_name(soup)
            publish_date = scraper.extract_publish_date(soup)
        except Exception as e:
            logger.warning(f"è·å–å…¬ä¼—å·ä¿¡æ¯å¤±è´¥: {e}")
            account_name = ""
            publish_date = ""
        
        # å¦‚æœéœ€è¦OCRï¼Œåœ¨åå°å¤„ç†å›¾ç‰‡
        ocr_text = ""
        if request.download_images and request.include_ocr:
            # åŒæ­¥å¤„ç†OCRï¼ˆä¸ºäº†ç¡®ä¿æ•°æ®å®Œæ•´æ€§ï¼‰
            for i, img_info in enumerate(article_info['images']):
                try:
                    file_extension = scraper._get_file_extension(img_info['src'])
                    filename = f"{config.IMAGE_FILENAME_PREFIX}{i+1}{file_extension}"
                    image_path = scraper.download_image(img_info['src'], filename)
                    
                    if image_path:
                        ocr_result = scraper.ocr_image(image_path)
                        article_info['ocr_results'].append({
                            'image_url': img_info['src'],
                            'local_path': image_path,
                            'ocr_text': ocr_result,
                            'alt': img_info['alt'],
                            'title': img_info['title']
                        })
                except Exception as e:
                    logger.error(f"å¤„ç†å›¾ç‰‡ {i+1} å¤±è´¥: {e}")
                    if not config.CONTINUE_ON_ERROR:
                        raise HTTPException(status_code=500, detail=f"å¤„ç†å›¾ç‰‡å¤±è´¥: {e}")
            
            # ç»„è£…OCRæ–‡æœ¬
            ocr_combined = []
            for i, item in enumerate(article_info.get('ocr_results', [])):
                ocr_combined.append(f"[å›¾ç‰‡{i+1}] {item.get('image_url','')}\n{item.get('ocr_text','')}")
            ocr_text = "\n\n".join(ocr_combined) if ocr_combined else ""
        
        # ç»„è£…å®Œæ•´å†…å®¹
        full_content = article_info['content']
        if ocr_text:
            full_content += "\n\nğŸ“¸ å›¾ç‰‡ OCR è¯†åˆ«å†…å®¹ï¼š\n" + ocr_text
        
        # åˆ›å»ºé£ä¹¦å®¢æˆ·ç«¯
        client = FeishuBitableClient(
            app_id=config.FEISHU_APP_ID,
            app_secret=config.FEISHU_APP_SECRET,
            app_token=config.FEISHU_APP_TOKEN,
            table_id=config.FEISHU_TABLE_ID,
            auth_mode=config.FEISHU_AUTH_MODE,
            bearer_token=config.FEISHU_BEARER_TOKEN,
        )
        
        # éªŒè¯è¡¨æ˜¯å¦å­˜åœ¨
        client.ensure_table_exists()
        
        # å‡†å¤‡å­—æ®µæ•°æ®
        fields_map = config.FEISHU_FIELDS
        fields = {
            fields_map.get('account_name', 'å…¬ä¼—å·åç§°'): account_name or '',
            fields_map.get('title', 'æ ‡é¢˜'): article_info['title'],
            fields_map.get('content', 'æ­£æ–‡'): full_content,
            fields_map.get('read_count', 'é˜…è¯»é‡'): '',
            fields_map.get('publish_date', 'å‘å¸ƒæ—¥æœŸ'): publish_date or '',
        }
        
        # å†™å…¥é£ä¹¦
        result = client.add_record(fields)
        
        logger.info(f"æˆåŠŸä¿å­˜æ–‡ç« åˆ°é£ä¹¦: {article_info['title']}")
        return ApiResponse(
            success=True,
            message="æ–‡ç« å·²æˆåŠŸä¿å­˜åˆ°é£ä¹¦",
            data={
                "title": article_info['title'],
                "account_name": account_name,
                "publish_date": publish_date,
                "content_length": len(full_content),
                "image_count": len(article_info['images']),
                "ocr_count": len(article_info['ocr_results']),
                "feishu_record_id": result.get('data', {}).get('record', {}).get('record_id', '')
            }
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ä¿å­˜åˆ°é£ä¹¦å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"ä¿å­˜åˆ°é£ä¹¦å¤±è´¥: {str(e)}")

@app.get("/health")
async def health_check():
    """å¥åº·æ£€æŸ¥æ¥å£"""
    return {"status": "healthy", "message": "æœåŠ¡è¿è¡Œæ­£å¸¸"}

if __name__ == "__main__":
    # å¯åŠ¨æœåŠ¡å™¨
    uvicorn.run(
        "api_server:app",
        host="0.0.0.0",
        port=8000,
        reload=True,
        log_level="info"
    )