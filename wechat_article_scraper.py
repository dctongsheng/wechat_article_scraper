#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¾®ä¿¡å…¬ä¼—å·æ–‡ç« æŠ“å–å™¨
åŠŸèƒ½ï¼š
1. æŠ“å–æ–‡ç« æ ‡é¢˜
2. æå–æ­£æ–‡çº¯æ–‡æœ¬å†…å®¹
3. æå–æ­£æ–‡ä¸­æ‰€æœ‰å›¾ç‰‡é“¾æ¥
4. ä¸‹è½½å›¾ç‰‡å¹¶ç”¨OCRè¯†åˆ«ä¸­æ–‡æ–‡å­—
5. è¾“å‡ºæ‰€æœ‰ä¿¡æ¯åˆ°æ§åˆ¶å°
"""

import requests
import re
import os
import time
from urllib.parse import urljoin, urlparse
from bs4 import BeautifulSoup
import json
from PIL import Image
import pytesseract
from io import BytesIO
import logging
import config

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class WeChatArticleScraper:
    """å¾®ä¿¡å…¬ä¼—å·æ–‡ç« æŠ“å–å™¨"""
    
    def __init__(self, output_dir="downloaded_images"):
        """
        åˆå§‹åŒ–æŠ“å–å™¨
        
        Args:
            output_dir (str): å›¾ç‰‡ä¸‹è½½ç›®å½•
        """
        self.session = requests.Session()
        self.output_dir = output_dir
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        }
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        if not os.path.exists(output_dir):
            os.makedirs(output_dir)
            logger.info(f"åˆ›å»ºå›¾ç‰‡ä¸‹è½½ç›®å½•: {output_dir}")
    
    def get_article_content(self, url):
        """
        è·å–æ–‡ç« å†…å®¹
        
        Args:
            url (str): å¾®ä¿¡å…¬ä¼—å·æ–‡ç« é“¾æ¥
            
        Returns:
            dict: åŒ…å«æ–‡ç« ä¿¡æ¯çš„å­—å…¸
        """
        try:
            logger.info(f"å¼€å§‹æŠ“å–æ–‡ç« : {url}")
            response = self.session.get(url, headers=self.headers, timeout=30)
            response.raise_for_status()
            response.encoding = 'utf-8'
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # æå–æ–‡ç« ä¿¡æ¯
            article_info = {
                'url': url,
                'title': self._extract_title(soup),
                'content': self._extract_content(soup),
                'images': self._extract_images(soup),
                'ocr_results': []
            }
            
            logger.info(f"æˆåŠŸæå–æ–‡ç« æ ‡é¢˜: {article_info['title']}")
            logger.info(f"æˆåŠŸæå–æ­£æ–‡å†…å®¹ï¼Œé•¿åº¦: {len(article_info['content'])} å­—ç¬¦")
            logger.info(f"å‘ç° {len(article_info['images'])} å¼ å›¾ç‰‡")
            
            return article_info
            
        except Exception as e:
            logger.error(f"æŠ“å–æ–‡ç« å¤±è´¥: {str(e)}")
            return None
    
    def _extract_title(self, soup):
        """æå–æ–‡ç« æ ‡é¢˜"""
        # å°è¯•å¤šç§æ–¹å¼æå–æ ‡é¢˜
        title_selectors = [
            'h1.rich_media_title',
            'h1[id*="activity-name"]',
            'h1',
            'title'
        ]
        
        for selector in title_selectors:
            title_elem = soup.select_one(selector)
            if title_elem:
                title = title_elem.get_text(strip=True)
                if title:
                    return title
        
        return "æœªæ‰¾åˆ°æ ‡é¢˜"
    
    def _extract_content(self, soup):
        """æå–æ–‡ç« æ­£æ–‡å†…å®¹"""
        # å°è¯•å¤šç§æ–¹å¼æå–æ­£æ–‡
        content_selectors = [
            'div.rich_media_content',
            'div[id*="js_content"]',
            'div.article-content',
            'div.content'
        ]
        
        for selector in content_selectors:
            content_elem = soup.select_one(selector)
            if content_elem:
                # ç§»é™¤è„šæœ¬å’Œæ ·å¼æ ‡ç­¾
                for script in content_elem(["script", "style"]):
                    script.decompose()
                
                # è·å–çº¯æ–‡æœ¬å†…å®¹
                content = content_elem.get_text(separator='\n', strip=True)
                # æ¸…ç†å¤šä½™çš„ç©ºç™½å­—ç¬¦
                content = re.sub(r'\n\s*\n', '\n\n', content)
                return content
        
        return "æœªæ‰¾åˆ°æ­£æ–‡å†…å®¹"
    
    def _extract_images(self, soup):
        """æå–æ–‡ç« ä¸­çš„å›¾ç‰‡é“¾æ¥"""
        images = []
        
        # æŸ¥æ‰¾æ‰€æœ‰å›¾ç‰‡æ ‡ç­¾
        img_tags = soup.find_all('img')
        
        for img in img_tags:
            src = img.get('src') or img.get('data-src')
            if src:
                # å¤„ç†ç›¸å¯¹URL
                if src.startswith('//'):
                    src = 'https:' + src
                elif src.startswith('/'):
                    src = 'https://mp.weixin.qq.com' + src
                
                # è¿‡æ»¤æ‰ä¸€äº›ä¸éœ€è¦çš„å›¾ç‰‡
                if not any(skip in src.lower() for skip in ['avatar', 'icon', 'logo', 'qrcode']):
                    images.append({
                        'src': src,
                        'alt': img.get('alt', ''),
                        'title': img.get('title', '')
                    })
        
        return images
    
    def download_image(self, image_url, filename):
        """
        ä¸‹è½½å›¾ç‰‡
        
        Args:
            image_url (str): å›¾ç‰‡URL
            filename (str): ä¿å­˜çš„æ–‡ä»¶å
            
        Returns:
            str: ä¸‹è½½çš„æ–‡ä»¶è·¯å¾„ï¼Œå¤±è´¥è¿”å›None
        """
        try:
            response = self.session.get(image_url, headers=self.headers, timeout=30)
            response.raise_for_status()
            
            file_path = os.path.join(self.output_dir, filename)
            
            with open(file_path, 'wb') as f:
                f.write(response.content)
            
            logger.info(f"æˆåŠŸä¸‹è½½å›¾ç‰‡: {filename}")
            return file_path
            
        except Exception as e:
            logger.error(f"ä¸‹è½½å›¾ç‰‡å¤±è´¥ {image_url}: {str(e)}")
            return None
    
    def ocr_image(self, image_path):
        """
        å¯¹å›¾ç‰‡è¿›è¡ŒOCRè¯†åˆ«
        
        Args:
            image_path (str): å›¾ç‰‡æ–‡ä»¶è·¯å¾„
            
        Returns:
            str: OCRè¯†åˆ«ç»“æœ
        """
        try:
            # æ‰“å¼€å›¾ç‰‡
            image = Image.open(image_path)
            
            # ä½¿ç”¨pytesseractè¿›è¡ŒOCRè¯†åˆ«
            # è®¾ç½®è¯­è¨€ä¸ºä¸­æ–‡ç®€ä½“
            text = pytesseract.image_to_string(image, lang='chi_sim')
            
            # æ¸…ç†è¯†åˆ«ç»“æœ
            text = text.strip()
            
            if text:
                logger.info(f"OCRè¯†åˆ«æˆåŠŸ: {image_path}")
                return text
            else:
                logger.warning(f"OCRè¯†åˆ«ç»“æœä¸ºç©º: {image_path}")
                return "OCRè¯†åˆ«ç»“æœä¸ºç©º"
                
        except Exception as e:
            logger.error(f"OCRè¯†åˆ«å¤±è´¥ {image_path}: {str(e)}")
            return f"OCRè¯†åˆ«å¤±è´¥: {str(e)}"
    
    def process_article(self, url):
        """
        å¤„ç†å®Œæ•´çš„æ–‡ç« æŠ“å–æµç¨‹
        
        Args:
            url (str): å¾®ä¿¡å…¬ä¼—å·æ–‡ç« é“¾æ¥
            
        Returns:
            dict: å®Œæ•´çš„æ–‡ç« ä¿¡æ¯
        """
        # 1. è·å–æ–‡ç« å†…å®¹
        article_info = self.get_article_content(url)
        if not article_info:
            return None
        
        # 2. ä¸‹è½½å›¾ç‰‡å¹¶è¿›è¡ŒOCRè¯†åˆ«
        for i, img_info in enumerate(article_info['images']):
            logger.info(f"å¤„ç†ç¬¬ {i+1} å¼ å›¾ç‰‡: {img_info['src']}")
            
            # ç”Ÿæˆæ–‡ä»¶å
            file_extension = self._get_file_extension(img_info['src'])
            filename = f"image_{i+1}{file_extension}"
            
            # ä¸‹è½½å›¾ç‰‡
            image_path = self.download_image(img_info['src'], filename)
            
            if image_path:
                # OCRè¯†åˆ«
                ocr_text = self.ocr_image(image_path)
                
                # ä¿å­˜OCRç»“æœ
                article_info['ocr_results'].append({
                    'image_url': img_info['src'],
                    'local_path': image_path,
                    'ocr_text': ocr_text,
                    'alt': img_info['alt'],
                    'title': img_info['title']
                })
                
                # æ·»åŠ å»¶è¿Ÿï¼Œé¿å…è¯·æ±‚è¿‡äºé¢‘ç¹
                time.sleep(1)
        
        return article_info
    
    def _get_file_extension(self, url):
        """ä»URLä¸­æå–æ–‡ä»¶æ‰©å±•å"""
        parsed = urlparse(url)
        path = parsed.path
        if '.' in path:
            return '.' + path.split('.')[-1]
        return '.jpg'  # é»˜è®¤æ‰©å±•å
    
    def print_results(self, article_info):
        """
        å°†ç»“æœè¾“å‡ºåˆ°æ§åˆ¶å°
        
        Args:
            article_info (dict): æ–‡ç« ä¿¡æ¯
        """
        if not article_info:
            print("æ²¡æœ‰å¯ç”¨çš„æ–‡ç« ä¿¡æ¯")
            return
        
        print("\n" + "="*80)
        print("å¾®ä¿¡å…¬ä¼—å·æ–‡ç« æŠ“å–ç»“æœ")
        print("="*80)
        
        # 1. æ–‡ç« æ ‡é¢˜
        print(f"\nğŸ“° æ–‡ç« æ ‡é¢˜:")
        print(f"   {article_info['title']}")
        
        # 2. æ–‡ç« é“¾æ¥
        print(f"\nğŸ”— æ–‡ç« é“¾æ¥:")
        print(f"   {article_info['url']}")
        
        # 3. æ­£æ–‡å†…å®¹
        print(f"\nğŸ“ æ­£æ–‡å†…å®¹:")
        content = article_info['content']
        if len(content) > 500:
            print(f"   {content[:500]}...")
            print(f"   (å†…å®¹è¿‡é•¿ï¼Œå·²æˆªå–å‰500å­—ç¬¦ï¼Œå®Œæ•´å†…å®¹è¯·æŸ¥çœ‹æ—¥å¿—)")
        else:
            print(f"   {content}")
        
        # 4. å›¾ç‰‡ä¿¡æ¯
        print(f"\nğŸ–¼ï¸  å›¾ç‰‡ä¿¡æ¯ (å…±{len(article_info['images'])}å¼ ):")
        for i, img in enumerate(article_info['images']):
            print(f"   å›¾ç‰‡ {i+1}:")
            print(f"     URL: {img['src']}")
            if img['alt']:
                print(f"     æè¿°: {img['alt']}")
            if img['title']:
                print(f"     æ ‡é¢˜: {img['title']}")
        
        # 5. OCRè¯†åˆ«ç»“æœ
        print(f"\nğŸ” OCRè¯†åˆ«ç»“æœ:")
        for i, ocr_result in enumerate(article_info['ocr_results']):
            print(f"   å›¾ç‰‡ {i+1} OCRç»“æœ:")
            print(f"     æœ¬åœ°è·¯å¾„: {ocr_result['local_path']}")
            print(f"     è¯†åˆ«æ–‡å­—: {ocr_result['ocr_text']}")
            print()
        
        print("="*80)
        print("æŠ“å–å®Œæˆï¼")

    def extract_account_name(self, soup):
        """å°è¯•æå–å…¬ä¼—å·åç§°"""
        selectors = [
            '#js_name',
            'a#js_name',
            '.rich_media_meta_list .rich_media_meta_nickname',
            'span.rich_media_meta_text',
        ]
        for sel in selectors:
            elem = soup.select_one(sel)
            if elem:
                name = elem.get_text(strip=True)
                if name:
                    return name
        return ""

    def extract_publish_date(self, soup):
        """å°è¯•æå–å‘å¸ƒæ—¥æœŸï¼ˆè¿”å›å­—ç¬¦ä¸²ï¼‰"""
        # å¾®ä¿¡å¸¸è§å‘å¸ƒæ—¶é—´åœ¨scripté‡Œæˆ–span#publish_time
        candidates = []
        span = soup.select_one('#publish_time')
        if span:
            candidates.append(span.get_text(strip=True))
        time_span = soup.select_one('span.rich_media_meta_text')
        if time_span:
            candidates.append(time_span.get_text(strip=True))
        for text in candidates:
            if text:
                return text
        return ""

# ================== é£ä¹¦å†™å…¥åŠŸèƒ½ ==================
class FeishuBitableClient:
    def __init__(self, app_id: str, app_secret: str, app_token: str, table_id: str, auth_mode: str = 'tenant', bearer_token: str | None = None):
        self.app_id = app_id
        self.app_secret = app_secret
        self.app_token = app_token
        self.table_id = table_id
        self.auth_mode = auth_mode
        self.bearer_token = bearer_token
        self.base = "https://open.feishu.cn/open-apis"
        self._app_access_token = None

    def _get_tenant_access_token(self) -> str:
        if self._app_access_token:
            return self._app_access_token
        url = f"{self.base}/auth/v3/tenant_access_token/internal/"
        resp = requests.post(url, json={
            "app_id": self.app_id,
            "app_secret": self.app_secret,
        }, timeout=30)
        data = resp.json()
        if data.get("code") != 0:
            raise RuntimeError(f"è·å–tenant_access_tokenå¤±è´¥: {data}")
        self._app_access_token = data["tenant_access_token"]
        return self._app_access_token

    def _get_authorization_header(self) -> dict:
        """æ ¹æ®é…ç½®è¿”å› Authorization å¤´ã€‚"""
        if self.auth_mode == 'user' and self.bearer_token:
            return {"Authorization": f"Bearer {self.bearer_token}"}
        # é»˜è®¤ä½¿ç”¨ç§Ÿæˆ· token
        return {"Authorization": f"Bearer {self._get_tenant_access_token()}"}

    def add_record(self, fields: dict) -> dict:
        url = f"{self.base}/bitable/v1/apps/{self.app_token}/tables/{self.table_id}/records"
        headers = {**self._get_authorization_header(), "Content-Type": "application/json"}
        payload = {"fields": fields}
        resp = requests.post(url, headers=headers, json=payload, timeout=30)
        data = resp.json()
        if resp.status_code != 200 or data.get("code", 0) != 0:
            raise RuntimeError(f"å†™å…¥é£ä¹¦å¤±è´¥: status={resp.status_code}, body={resp.text}")
        return data

    def list_tables(self) -> list:
        """åˆ—å‡º base ä¸‹æ‰€æœ‰è¡¨ï¼ˆç”¨äºè¯Šæ–­ app_token æ˜¯å¦æ­£ç¡®ï¼‰"""
        url = f"{self.base}/bitable/v1/apps/{self.app_token}/tables"
        headers = self._get_authorization_header()
        resp = requests.get(url, headers=headers, timeout=30)
        data = resp.json()
        if resp.status_code != 200 or data.get("code", 0) != 0:
            raise RuntimeError(f"è·å–è¡¨åˆ—è¡¨å¤±è´¥: status={resp.status_code}, body={resp.text}")
        return data.get("data", {}).get("items", [])

    def ensure_table_exists(self):
        """æ ¡éªŒ table_id æ˜¯å¦å­˜åœ¨ï¼›ä¸å­˜åœ¨åˆ™æŠ›é”™å¹¶æ‰“å°å¯ç”¨è¡¨ä¿¡æ¯"""
        tables = self.list_tables()
        table_ids = [t.get('table_id') or t.get('id') for t in tables]
        if self.table_id not in table_ids:
            names = [f"{t.get('name')}({t.get('table_id') or t.get('id')})" for t in tables]
            raise RuntimeError(
                "è¡¨IDä¸å­˜åœ¨æˆ–ä¸åœ¨è¯¥å¤šç»´è¡¨æ ¼ä¸‹ã€‚è¯·ç¡®è®¤ FEISHU_APP_TOKEN ä¸ FEISHU_TABLE_ID æ˜¯å¦å¯¹åº”åŒä¸€ä¸ªå¤šç»´è¡¨æ ¼ã€‚" \
                + (" å¯ç”¨è¡¨: " + ", ".join(names) if names else " æœªè·å–åˆ°ä»»ä½•è¡¨ï¼Œè¯·æ£€æŸ¥ App Token æ˜¯å¦æ­£ç¡®ã€åº”ç”¨æ˜¯å¦æœ‰è®¿é—®æƒé™")
            )


def main():
    """ä¸»å‡½æ•°"""
    # ä»é…ç½®è¯»å–æ–‡ç« é“¾æ¥
    article_url = getattr(config, 'ARTICLE_URL', "https://mp.weixin.qq.com/s/w0h03IsxfSrour3NpE_FqA")
    
    print("å¾®ä¿¡å…¬ä¼—å·æ–‡ç« æŠ“å–å™¨")
    print("="*50)
    
    # åˆ›å»ºæŠ“å–å™¨å®ä¾‹
    scraper = WeChatArticleScraper()
    
    # å¤„ç†æ–‡ç« 
    article_info = scraper.process_article(article_url)
    
    # è¾“å‡ºç»“æœ
    if article_info:
        scraper.print_results(article_info)
        
        # å†™å…¥é£ä¹¦
        if getattr(config, 'FEISHU_ENABLED', False):
            try:
                # è§£æå…¬ä¼—å·åä¸å‘å¸ƒæ—¥æœŸ
                # é‡æ–°è·å–soupä»¥æŠ½å–è´¦å·/æ—¥æœŸï¼ˆä¹Ÿå¯åœ¨get_article_contentä¸­ä¿ç•™soupï¼Œè¿™é‡Œä¸ºç®€æ´é‡å¤è¯·æ±‚ä¸€æ¬¡ï¼‰
                html = scraper.session.get(article_url, headers=scraper.headers, timeout=30)
                html.raise_for_status()
                soup = BeautifulSoup(html.text, 'html.parser')
                account_name = scraper.extract_account_name(soup)
                publish_date = scraper.extract_publish_date(soup)

                fields_map = getattr(config, 'FEISHU_FIELDS', {})
                client = FeishuBitableClient(
                    app_id=getattr(config, 'FEISHU_APP_ID', ''),
                    app_secret=getattr(config, 'FEISHU_APP_SECRET', ''),
                    app_token=getattr(config, 'FEISHU_APP_TOKEN', ''),
                    table_id=getattr(config, 'FEISHU_TABLE_ID', ''),
                    auth_mode=getattr(config, 'FEISHU_AUTH_MODE', 'tenant'),
                    bearer_token=getattr(config, 'FEISHU_BEARER_TOKEN', None),
                )
                # å…ˆæ ¡éªŒ token ä¸è¡¨æ˜¯å¦å­˜åœ¨
                client.ensure_table_exists()
                # ç»„è£…æ­£æ–‡ + OCRæ‘˜è¦
                ocr_combined = []
                for i, item in enumerate(article_info.get('ocr_results', [])):
                    ocr_combined.append(f"[å›¾ç‰‡{i+1}] {item.get('image_url','')}\n{item.get('ocr_text','')}")
                ocr_text = "\n\n".join(ocr_combined) if ocr_combined else ""
                full_content = article_info['content'] + ("\n\nğŸ“¸ å›¾ç‰‡ OCR è¯†åˆ«å†…å®¹ï¼š\n" + ocr_text if ocr_text else "")

                fields = {
                    fields_map.get('account_name', 'å…¬ä¼—å·åç§°'): account_name or '',
                    fields_map.get('title', 'æ ‡é¢˜'): article_info['title'],
                    fields_map.get('content', 'æ­£æ–‡'): full_content,
                    fields_map.get('read_count', 'é˜…è¯»é‡'): '',
                    fields_map.get('publish_date', 'å‘å¸ƒæ—¥æœŸ'): publish_date or '',
                }
                client.add_record(fields)
                print("âœ… å·²å†™å…¥é£ä¹¦å¤šç»´è¡¨æ ¼")
            except Exception as e:
                print(f"âš ï¸ å†™å…¥é£ä¹¦å¤±è´¥: {e}")
    else:
        print("æŠ“å–å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥å’Œæ–‡ç« é“¾æ¥")


if __name__ == "__main__":
    main()
